{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-10T07:25:48.420576Z",
     "iopub.status.busy": "2025-02-10T07:25:48.420263Z",
     "iopub.status.idle": "2025-02-10T07:25:49.672687Z",
     "shell.execute_reply": "2025-02-10T07:25:49.671856Z",
     "shell.execute_reply.started": "2025-02-10T07:25:48.420548Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = user_secrets.get_secret(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "os.environ['WANDB_API_KEY'] = user_secrets.get_secret(\"WANDB_API_KEY \")\n",
    "\n",
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "hf_token = user_secrets.get_secret(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "login(token = hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T07:25:49.673931Z",
     "iopub.status.busy": "2025-02-10T07:25:49.673690Z",
     "iopub.status.idle": "2025-02-10T07:25:49.679567Z",
     "shell.execute_reply": "2025-02-10T07:25:49.678601Z",
     "shell.execute_reply.started": "2025-02-10T07:25:49.673911Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T07:25:49.746467Z",
     "iopub.status.busy": "2025-02-10T07:25:49.746167Z",
     "iopub.status.idle": "2025-02-10T07:25:49.984798Z",
     "shell.execute_reply": "2025-02-10T07:25:49.983526Z",
     "shell.execute_reply.started": "2025-02-10T07:25:49.746441Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mkdir src\n",
    "!mkdir src/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T07:25:49.987829Z",
     "iopub.status.busy": "2025-02-10T07:25:49.987557Z",
     "iopub.status.idle": "2025-02-10T07:25:49.993690Z",
     "shell.execute_reply": "2025-02-10T07:25:49.992923Z",
     "shell.execute_reply.started": "2025-02-10T07:25:49.987777Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/utils/exp_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/exp_utils.py\n",
    "import os\n",
    "\n",
    "def create_exp_dir(exp_name):\n",
    "    os.makedirs('exps', exist_ok=True)\n",
    "    exp_dir = os.path.join('exps', exp_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "\n",
    "    sub_dirs = ['checkpoints', 'configs', 'data', 'results']\n",
    "\n",
    "    for dir in sub_dirs:\n",
    "        dir_path = os.path.join(exp_dir, dir)\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "    \n",
    "    results_dir = os.path.join(exp_dir, 'results')\n",
    "    checkpoints_dir = os.path.join(exp_dir, 'checkpoints')\n",
    "    data_dir = os.path.join(exp_dir, 'data')\n",
    "    configs_dir = os.path.join(exp_dir, 'configs')\n",
    "\n",
    "    return (\n",
    "        exp_dir, \n",
    "        configs_dir,\n",
    "        data_dir, \n",
    "        checkpoints_dir,\n",
    "        results_dir\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def summarize_results():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T07:25:49.995234Z",
     "iopub.status.busy": "2025-02-10T07:25:49.994958Z",
     "iopub.status.idle": "2025-02-10T07:25:50.011920Z",
     "shell.execute_reply": "2025-02-10T07:25:50.011120Z",
     "shell.execute_reply.started": "2025-02-10T07:25:49.995205Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/utils/log_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/log_utils.py\n",
    "import logging\n",
    "from colorama import Fore, Style, init\n",
    "\n",
    "def init_logging() -> None:\n",
    "    \"\"\"Initialize logging with colored output.\"\"\"\n",
    "    init(autoreset=True)  # Initialize colorama for cross-platform compatibility\n",
    "\n",
    "    class ColorFormatter(logging.Formatter):\n",
    "        \"\"\"Custom formatter to add colors to log levels.\"\"\"\n",
    "        LOG_COLORS = {\n",
    "            logging.DEBUG: Fore.CYAN,\n",
    "            logging.INFO: Fore.GREEN,\n",
    "            logging.WARNING: Fore.YELLOW,\n",
    "            logging.ERROR: Fore.RED,\n",
    "            logging.CRITICAL: Fore.RED + Style.BRIGHT,\n",
    "        }\n",
    "\n",
    "        def format(self, record):\n",
    "            log_color = self.LOG_COLORS.get(record.levelno, \"\")\n",
    "            record.levelname = f\"{log_color}{record.levelname}{Style.RESET_ALL}\"\n",
    "            return super().format(record)\n",
    "\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = ColorFormatter(\n",
    "        fmt=\"%(asctime)s %(levelname)s: %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO, handlers=[handler])\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     init_logging()\n",
    "#     logging.debug(\"This is a debug message\")\n",
    "#     logging.info(\"This is an info message\")\n",
    "#     logging.warning(\"This is a warning message\")\n",
    "#     logging.error(\"This is an error message\")\n",
    "#     logging.critical(\"This is a critical message\")\n",
    "\n",
    "\n",
    "import logging\n",
    "from colorama import Fore, Style, init\n",
    "\n",
    "# Initialize colorama\n",
    "init(autoreset=True)\n",
    "\n",
    "class ColorFormatter(logging.Formatter):\n",
    "    \"\"\"Custom formatter to add colors to log levels.\"\"\"\n",
    "    \n",
    "    LOG_COLORS = {\n",
    "        logging.DEBUG: Fore.CYAN,\n",
    "        logging.INFO: Fore.GREEN,\n",
    "        logging.WARNING: Fore.YELLOW,\n",
    "        logging.ERROR: Fore.RED,\n",
    "        logging.CRITICAL: Fore.RED + Style.BRIGHT,\n",
    "    }\n",
    "\n",
    "    def format(self, record):\n",
    "        log_color = self.LOG_COLORS.get(record.levelno, \"\")\n",
    "        record.levelname = f\"{log_color}{record.levelname}{Style.RESET_ALL}\"\n",
    "        return super().format(record)\n",
    "\n",
    "def setup_logger(name: str):\n",
    "    \"\"\"Set up a logger with colored output.\"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    handler = logging.StreamHandler()\n",
    "    formatter = ColorFormatter(\n",
    "        \"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    "    handler.setFormatter(formatter)\n",
    "    logger.addHandler(handler)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "    return logger\n",
    "\n",
    "# # Example usage\n",
    "# logger = setup_logger(\"my_logger\")\n",
    "# logger.debug(\"This is a debug message\")\n",
    "# logger.info(\"This is an info message\")\n",
    "# logger.warning(\"This is a warning message\")\n",
    "# logger.error(\"This is an error message\")\n",
    "# logger.critical(\"This is a critical message\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T07:25:50.013116Z",
     "iopub.status.busy": "2025-02-10T07:25:50.012820Z",
     "iopub.status.idle": "2025-02-10T07:25:50.029713Z",
     "shell.execute_reply": "2025-02-10T07:25:50.028872Z",
     "shell.execute_reply.started": "2025-02-10T07:25:50.013089Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prepare_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prepare_data.py\n",
    "# prepare_data.py\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "def tokenize_captions(examples, tokenizer, caption_column):\n",
    "    captions = []\n",
    "    for caption in examples[caption_column]:\n",
    "        if isinstance(caption, str):\n",
    "            captions.append(caption)\n",
    "    inputs = tokenizer(\n",
    "        captions,\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return inputs.input_ids\n",
    "\n",
    "def get_train_transforms(resolution, center_crop=True, random_flip=True):\n",
    "    transform_list = [\n",
    "        transforms.Resize(resolution, interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "    ]\n",
    "    if center_crop:\n",
    "        transform_list.append(transforms.CenterCrop(resolution))\n",
    "    else:\n",
    "        transform_list.append(transforms.RandomCrop(resolution))\n",
    "    if random_flip:\n",
    "        transform_list.append(transforms.RandomHorizontalFlip())\n",
    "    else:\n",
    "        transform_list.append(transforms.Lambda(lambda x: x))\n",
    "    transform_list.extend([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5])\n",
    "    ])\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "def preprocess_train(examples, tokenizer, image_column, caption_column, train_transforms):\n",
    "    images = [image.convert(\"RGB\") for image in examples[image_column]]\n",
    "    examples[\"pixel_values\"] = [train_transforms(image) for image in images]\n",
    "    examples[\"input_ids\"] = tokenize_captions(examples, tokenizer, caption_column)\n",
    "    return examples\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "    input_ids = torch.stack([example[\"input_ids\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"input_ids\": input_ids}\n",
    "\n",
    "def prepare_dataset(dataset_name, train_data_dir, n_train_samples, tokenizer):\n",
    "    if dataset_name:\n",
    "        dataset = load_dataset(dataset_name)\n",
    "    else:\n",
    "        dataset = load_dataset(\"imagefolder\", data_dir=train_data_dir)\n",
    "    train_data = dataset[\"train\"]\n",
    "    if train_n_samples>0:\n",
    "        dataset[\"train\"] = train_data.select(range(train_n_samples))\n",
    "    return dataset\n",
    "\n",
    "def get_dataloader(dataset, tokenizer, resolution, center_crop, random_flip, batch_size):\n",
    "    dataset_columns = list(dataset[\"train\"].features.keys())\n",
    "    image_column, caption_column = dataset_columns[0], dataset_columns[1]\n",
    "    train_transforms = get_train_transforms(resolution, center_crop, random_flip)\n",
    "    def transform_fn(examples):\n",
    "        return preprocess_train(examples, tokenizer, image_column, caption_column, train_transforms)\n",
    "    train_dataset = dataset[\"train\"].with_transform(transform_fn)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=0\n",
    "    )\n",
    "    return dataloader, image_column, caption_column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T08:25:19.149006Z",
     "iopub.status.busy": "2025-02-10T08:25:19.148588Z",
     "iopub.status.idle": "2025-02-10T08:25:19.155080Z",
     "shell.execute_reply": "2025-02-10T08:25:19.154022Z",
     "shell.execute_reply.started": "2025-02-10T08:25:19.148975Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils.py\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from peft import LoraConfig\n",
    "from peft.utils import get_peft_model_state_dict\n",
    "from diffusers.utils import convert_state_dict_to_diffusers\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "def get_formatted_date():\n",
    "    return datetime.now().strftime(r'%Y%m%d-%H%M%S')\n",
    "\n",
    "def get_lora_config(lora_rank, lora_alpha, target_modules):\n",
    "    return LoraConfig(\n",
    "        r=lora_rank,\n",
    "        lora_alpha=lora_alpha,\n",
    "        init_lora_weights=\"gaussian\",\n",
    "        target_modules=target_modules # [\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"]\n",
    "    )\n",
    "\n",
    "def freeze_parameters(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "def save_lora_weights(unet, output_dir, pretrained_model_name_or_path, lora_rank, max_train_steps, resolution, formatted_date, accelerator):\n",
    "    unet = unet.to(torch.float32)\n",
    "    unwrapped_unet = accelerator.unwrap_model(unet)\n",
    "    unet_lora_state_dict = convert_state_dict_to_diffusers(get_peft_model_state_dict(unwrapped_unet))\n",
    "    weight_name = f\"lora_{pretrained_model_name_or_path.split('/')[-1]}_rank{lora_rank}_s{max_train_steps}_r{resolution}_{formatted_date}.safetensors\"\n",
    "    StableDiffusionPipeline.save_lora_weights(\n",
    "        save_directory=output_dir,\n",
    "        unet_lora_layers=unet_lora_state_dict,\n",
    "        safe_serialization=True,\n",
    "        weight_name=weight_name\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T09:46:19.980737Z",
     "iopub.status.busy": "2025-02-10T09:46:19.980386Z",
     "iopub.status.idle": "2025-02-10T09:46:19.988832Z",
     "shell.execute_reply": "2025-02-10T09:46:19.988091Z",
     "shell.execute_reply.started": "2025-02-10T09:46:19.980713Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting finetune.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile finetune.py\n",
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "finetune.py\n",
    "\n",
    "Fine-tuning script for Stable Diffusion with LoRA adaptation.\n",
    "This script loads experiment configuration using Hydra, sets up the experiment\n",
    "environment, loads the model and dataset, and runs the training loop.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from omegaconf import OmegaConf\n",
    "from hydra import initialize, compose\n",
    "from hydra.utils import instantiate\n",
    "\n",
    "from accelerate import Accelerator, utils as accel_utils\n",
    "from transformers import set_seed\n",
    "\n",
    "from diffusers import DDPMScheduler, StableDiffusionPipeline\n",
    "from diffusers.optimization import get_scheduler\n",
    "\n",
    "# Local modules\n",
    "from prepare_data import prepare_dataset, get_dataloader\n",
    "from utils import get_formatted_date, get_lora_config, freeze_parameters, save_lora_weights\n",
    "from src.utils.log_utils import setup_logger\n",
    "from src.utils.exp_utils import create_exp_dir\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for fine-tuning Stable Diffusion with LoRA adaptation.\"\"\"\n",
    "    logger = setup_logger(\"ft_llm\")\n",
    "    logger.info(\"Setting up environment...\")\n",
    "\n",
    "    # Parse command-line arguments for configuration.\n",
    "    parser = argparse.ArgumentParser(description=\"Process experiment configurations.\")\n",
    "    parser.add_argument(\n",
    "        \"--config_path\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to the configuration file for the experiment.\",\n",
    "    )\n",
    "    args, override_args = parser.parse_known_args()\n",
    "\n",
    "    # Normalize and validate configuration path.\n",
    "    config_path = os.path.normpath(args.config_path)\n",
    "    if not os.path.isfile(config_path):\n",
    "        raise FileNotFoundError(f\"Configuration file not found at: {config_path}\")\n",
    "    config_dir = os.path.dirname(config_path)\n",
    "    config_fn = os.path.splitext(os.path.basename(config_path))[0]\n",
    "\n",
    "    # Load configuration using Hydra.\n",
    "    try:\n",
    "        with initialize(version_base=None, config_path=config_dir):\n",
    "            cfg = compose(config_name=config_fn, overrides=override_args)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load configuration from {config_path}: {e}\")\n",
    "\n",
    "    logger.info(\"Loaded configuration:\\n%s\", OmegaConf.to_yaml(cfg))\n",
    "\n",
    "    # Ensure experiment name consistency.\n",
    "    expected_exp_name = os.path.basename(config_path).replace(\".yaml\", \"\")\n",
    "    assert expected_exp_name == cfg.exp_manager.exp_name, (\n",
    "        f\"Experiment name mismatch: expected {expected_exp_name} but got {cfg.exp_manager.exp_name}\"\n",
    "    )\n",
    "\n",
    "    # Create experiment directories and copy the config file.\n",
    "    logger.info(\"Creating experiment directories...\")\n",
    "    exp_name = cfg.exp_manager.exp_name\n",
    "    exp_dir, configs_dir, data_dir, checkpoints_dir, results_dir = create_exp_dir(exp_name)\n",
    "    shutil.copy(config_path, configs_dir)\n",
    "\n",
    "    # Extract configuration parameters.\n",
    "    exp_args = cfg.exp_manager\n",
    "    train_args = cfg.train\n",
    "    data_args = cfg.prepare_data\n",
    "    model_args = cfg.prepare_model\n",
    "\n",
    "    # Set random seed for reproducibility.\n",
    "    seed = exp_args.seed if \"seed\" in exp_args else 2025\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Setup Accelerator.\n",
    "    accel_utils.write_basic_config()\n",
    "    accelerator = Accelerator(\n",
    "        log_with=\"wandb\",\n",
    "        gradient_accumulation_steps=train_args.train_args.gradient_accumulation_steps, \n",
    "        mixed_precision=\"fp16\"\n",
    "    )\n",
    "    device = accelerator.device\n",
    "    logger.info(f\"Device: {device}\")\n",
    "\n",
    "    # Load scheduler, tokenizer, and models.\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(model_args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "    weight_dtype = torch.float16\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        model_args.pretrained_model_name_or_path, torch_dtype=weight_dtype\n",
    "    ).to(device)\n",
    "    tokenizer = pipe.tokenizer\n",
    "    text_encoder = pipe.text_encoder\n",
    "    vae = pipe.vae\n",
    "    unet = pipe.unet\n",
    "\n",
    "    # Freeze parameters of VAE, text encoder, and unet (except LoRA adapters).\n",
    "    freeze_parameters(unet)\n",
    "    freeze_parameters(vae)\n",
    "    freeze_parameters(text_encoder)\n",
    "\n",
    "    # Configure and add LoRA adapter to unet.\n",
    "    unet_lora_config = get_lora_config(model_args.lora.r, \n",
    "                                       model_args.lora.lora_alpha, \n",
    "                                       model_args.lora.target_modules)\n",
    "    unet.add_adapter(unet_lora_config)\n",
    "    for param in unet.parameters():\n",
    "        if param.requires_grad:\n",
    "            param.data = param.to(torch.float32)\n",
    "\n",
    "    # Prepare dataset and dataloader.\n",
    "    dataset = prepare_dataset(\n",
    "        data_args.dataset.dataset_name,\n",
    "        data_args.dataset.train_data_dir,\n",
    "        data_args.dataset.train_n_samples,\n",
    "        tokenizer,\n",
    "    )\n",
    "    train_dataloader, _, _ = get_dataloader(\n",
    "        dataset,\n",
    "        tokenizer,\n",
    "        data_args.image.resolution,\n",
    "        data_args.image.center_crop,\n",
    "        data_args.image.random_flip,\n",
    "        train_args.train_args.per_device_train_batch_size,\n",
    "    )\n",
    "    # logger.info(\"Data Size: %d\", len(train_dataloader))\n",
    "\n",
    "    # max_train_steps = train_args.train_args.num_train_epochs * len(train_dataloader)\n",
    "\n",
    "    # Scheduler and math around the number of training steps.\n",
    "    overrode_max_train_steps = False\n",
    "    \n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / train_args.train_args.gradient_accumulation_steps)\n",
    "    if train_args.train_args.max_train_steps is None:\n",
    "        train_args.train_args.max_train_steps = train_args.train_args.num_train_epochs * num_update_steps_per_epoch\n",
    "        overrode_max_train_steps = True\n",
    "\n",
    "    # Initialize optimizer and learning rate scheduler.\n",
    "    lora_layers = filter(lambda p: p.requires_grad, unet.parameters())\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        lora_layers,\n",
    "        lr=train_args.optimizer.learning_rate,\n",
    "        betas=(train_args.optimizer.adam_beta1, train_args.optimizer.adam_beta2),\n",
    "        weight_decay=train_args.optimizer.adam_weight_decay,\n",
    "        eps=train_args.optimizer.adam_epsilon,\n",
    "    )\n",
    "\n",
    "\n",
    "    lr_scheduler = get_scheduler(\n",
    "        train_args.train_args.lr_scheduler_name,\n",
    "        optimizer=optimizer,\n",
    "        # num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,\n",
    "        # num_training_steps=args.max_train_steps * accelerator.num_processes,\n",
    "        # num_cycles=args.lr_num_cycles,\n",
    "        # power=args.lr_power,\n",
    "    )\n",
    "\n",
    "    # Prepare everything with our `accelerator`.\n",
    "    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        unet, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / train_args.train_args.gradient_accumulation_steps)\n",
    "    if overrode_max_train_steps:\n",
    "        train_args.train_args.max_train_steps = train_args.train_args.num_train_epochs * num_update_steps_per_epoch\n",
    "    \n",
    "    # Afterwards we recalculate our number of training epochs\n",
    "    train_args.train_args.num_train_epochs = math.ceil(train_args.train_args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "\n",
    "    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        unet, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    # Initialise your wandb run, passing wandb parameters and any config information\n",
    "    accelerator.init_trackers(\n",
    "        project_name=cfg.exp_manager.wandb.project\n",
    "        )\n",
    "\n",
    "    # Train!\n",
    "    total_batch_size = train_args.train_args.per_device_train_batch_size * accelerator.num_processes * train_args.train_args.gradient_accumulation_steps\n",
    "\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(dataset)}\")\n",
    "    logger.info(f\"  Num batches each epoch = {len(train_dataloader)}\")\n",
    "    logger.info(f\"  Num Epochs = {train_args.train_args.num_train_epochs}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {train_args.train_args.per_device_train_batch_size}\")\n",
    "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "    logger.info(f\"  Gradient Accumulation steps = {train_args.train_args.gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  Total optimization steps = {train_args.train_args.max_train_steps}\")\n",
    "\n",
    "    # import wandb\n",
    "    # wandb.init(\n",
    "    #     project=cfg.exp_manager.wandb.project,\n",
    "    #     # name = cfg.exp_manager.exp_name\n",
    "    # )\n",
    "    global_step = 0\n",
    "    first_epoch = 0\n",
    "\n",
    "    initial_global_step = 0\n",
    "\n",
    "    progress_bar = tqdm(\n",
    "        range(train_args.train_args.max_train_steps), \n",
    "        initial=initial_global_step,\n",
    "        desc=\"Steps\", \n",
    "        disable=not accelerator.is_local_main_process\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Training loop.\n",
    "    for epoch in range(first_epoch, train_args.train_args.num_train_epochs):\n",
    "        unet.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            models_to_accumulate = [unet]\n",
    "            with accelerator.accumulate(models_to_accumulate):\n",
    "                # Encode images into latent space.\n",
    "                latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\n",
    "                latents = latents * vae.config.scaling_factor\n",
    "    \n",
    "                # Add noise.\n",
    "                noise = torch.randn_like(latents)\n",
    "                batch_size = latents.shape[0]\n",
    "                timesteps = torch.randint(\n",
    "                    low=0,\n",
    "                    high=noise_scheduler.config.num_train_timesteps,\n",
    "                    size=(batch_size,),\n",
    "                    device=latents.device,\n",
    "                ).long()\n",
    "    \n",
    "                # Get text embeddings for conditioning.\n",
    "                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "    \n",
    "                # Determine target based on prediction type.\n",
    "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                    target = noise\n",
    "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "    \n",
    "                # Forward pass.\n",
    "                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "    \n",
    "                # Compute loss and perform backpropagation.\n",
    "                loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "                accelerator.backward(loss)\n",
    "                \n",
    "                if accelerator.sync_gradients:\n",
    "                    accelerator.clip_grad_norm_(lora_layers, train_args.train_args.max_grad_norm)\n",
    "                \n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "\n",
    "            if accelerator.is_main_process:\n",
    "                if train_args.train_args.checkpointing_steps:\n",
    "                    if global_step % train_args.train_args.checkpointing_steps == 0:\n",
    "                        save_path = os.path.join(checkpoints_dir, f\"checkpoint-{global_step}\")\n",
    "                        accelerator.save_state(save_path)\n",
    "                        logger.info(f\"Saved state to {save_path}\")\n",
    "            \n",
    "            logs = {\n",
    "                \"epoch\": epoch,\n",
    "                \"step_loss\": loss.detach().item(),\n",
    "                \"lr\": lr_scheduler.get_last_lr()[0],\n",
    "            }\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "\n",
    "\n",
    "    # Save the lora layers\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        save_lora_weights(\n",
    "            unet,\n",
    "            results_dir,\n",
    "            model_args.pretrained_model_name_or_path,\n",
    "            model_args.lora.r,\n",
    "            train_args.train_args.max_train_steps,\n",
    "            data_args.image.resolution,\n",
    "            get_formatted_date(),\n",
    "            accelerator,\n",
    "        )\n",
    "\n",
    "\n",
    "    # Log exp artifact\n",
    "    if exp_args.wandb.log_artifact == True:\n",
    "        logger.info(\"LOGGING EXP ARTIFACTS...\")\n",
    "        # Create an artifact\n",
    "        import wandb\n",
    "        artifact = wandb.Artifact(\n",
    "            name=exp_args.exp_name, \n",
    "            type=\"exp\", \n",
    "        )\n",
    "\n",
    "        # Add the directory to the artifact\n",
    "        artifact.add_dir(exp_dir)\n",
    "\n",
    "        # wandb_tracker = accelerator.get_tracker(\"wandb\")\n",
    "        # wandb_tracker.log_artifact(artifact)\n",
    "        \n",
    "        wandb.log_artifact(artifact)\n",
    "\n",
    "    # # Finish the W&B run\n",
    "    # wandb.finish()\n",
    "\n",
    "    accelerator.end_training()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T11:27:10.941259Z",
     "iopub.status.busy": "2025-02-10T11:27:10.940834Z",
     "iopub.status.idle": "2025-02-10T11:27:10.948500Z",
     "shell.execute_reply": "2025-02-10T11:27:10.947477Z",
     "shell.execute_reply.started": "2025-02-10T11:27:10.941214Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting generate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile generate.py\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler\n",
    "from diffusers.utils import make_image_grid\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Generate images using Stable Diffusion with LoRA adaptation.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--config_path\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Path to the configuration YAML file.\",\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Load configuration from the specified YAML file.\n",
    "    config = OmegaConf.load(args.config_path)\n",
    "    model_cfg = config.model\n",
    "    gen_cfg = config.generate\n",
    "\n",
    "    # Extract model parameters.\n",
    "    model_name_or_path = model_cfg.model_name_or_path\n",
    "    # lora_name = model_cfg.lora_name\n",
    "    # output_dir = model_cfg.output_dir\n",
    "    # lora_model_path = os.path.join(output_dir, lora_name)\n",
    "    lora_path = model_cfg.lora_path\n",
    "\n",
    "    # Extract generation parameters.\n",
    "    prompt = list(gen_cfg.prompt)\n",
    "    negative_prompt = gen_cfg.negative_prompt\n",
    "    num_images_per_prompt = gen_cfg.num_images_per_prompt\n",
    "    generator_seed = gen_cfg.generator_seed\n",
    "    width = gen_cfg.width\n",
    "    height = gen_cfg.height\n",
    "    guidance_scale = gen_cfg.guidance_scale\n",
    "    scheduler_type = gen_cfg.scheduler\n",
    "\n",
    "    # Set device.\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # Load the base Stable Diffusion pipeline.\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "    ).to(device)\n",
    "\n",
    "    if lora_path:\n",
    "        # lora_model_path = os.path.join(output_dir, lora_name)\n",
    "        # Load LoRA weights.\n",
    "        print(\"Loading LoRA Adapter...\")\n",
    "        pipe.load_lora_weights(\n",
    "            pretrained_model_name_or_path_or_dict=lora_path,\n",
    "            adapter_name=\"az_lora\"\n",
    "        )\n",
    "\n",
    "        # Activate the LoRA adapter.\n",
    "        pipe.set_adapters([\"az_lora\"], adapter_weights=[1.0])\n",
    "\n",
    "    # Configure scheduler.\n",
    "    if scheduler_type == \"EulerDiscreteScheduler\":\n",
    "        pipe.scheduler = EulerDiscreteScheduler.from_config(pipe.scheduler.config)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported scheduler type: {scheduler_type}\")\n",
    "\n",
    "    # Create a random generator for reproducibility.\n",
    "    generator = torch.Generator(device).manual_seed(generator_seed)\n",
    "\n",
    "    # Generate images.\n",
    "    output = pipe(\n",
    "        prompt=prompt,\n",
    "        # negative_prompt=negative_prompt,\n",
    "        num_images_per_prompt=num_images_per_prompt,\n",
    "        generator=generator,\n",
    "        width=width,\n",
    "        height=height,\n",
    "        guidance_scale=guidance_scale\n",
    "    )\n",
    "    images = output.images\n",
    "\n",
    "    # Create and save an image grid.\n",
    "    rows = len(prompt)\n",
    "    import math\n",
    "    cols = num_images_per_prompt # math.ceil(rows * num_images_per_prompt / rows)\n",
    "    grid = make_image_grid(images, rows=rows, cols=cols)\n",
    "    grid.save(\"output.png\")\n",
    "    print(\"Output image saved as output.png\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T07:25:50.080330Z",
     "iopub.status.busy": "2025-02-10T07:25:50.080063Z",
     "iopub.status.idle": "2025-02-10T07:25:50.212647Z",
     "shell.execute_reply": "2025-02-10T07:25:50.211490Z",
     "shell.execute_reply.started": "2025-02-10T07:25:50.080301Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mkdir configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T09:47:38.946708Z",
     "iopub.status.busy": "2025-02-10T09:47:38.946384Z",
     "iopub.status.idle": "2025-02-10T09:47:38.952878Z",
     "shell.execute_reply": "2025-02-10T09:47:38.952010Z",
     "shell.execute_reply.started": "2025-02-10T09:47:38.946680Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting configs/ft_sd15_lora.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile configs/ft_sd15_lora.yaml\n",
    "exp_manager:\n",
    "  exp_name: \"ft_sd15_lora\"\n",
    "  seed: 202502\n",
    "  task_name: \"stable_diffusion_finetune\"\n",
    "  model_name: \"sd-legacy/stable-diffusion-v1-5\"\n",
    "  dataset_name: \"yirenlu/heroicons\"\n",
    "  wandb:\n",
    "    use_wandb: true\n",
    "    project: \"text2icon_ft_sd\"\n",
    "    log_artifact: true\n",
    "    artifact_types: ['exp', 'data', 'configs', 'results', 'checkpoints']\n",
    "\n",
    "prepare_data:\n",
    "  dataset:\n",
    "    dataset_name: \"yirenlu/heroicons\"\n",
    "    train_data_dir: \n",
    "    train_n_samples: -1\n",
    "  image:\n",
    "    resolution: 256\n",
    "    center_crop: true\n",
    "    random_flip: true\n",
    "\n",
    "prepare_model:\n",
    "  pretrained_model_name_or_path: \"sd-legacy/stable-diffusion-v1-5\"\n",
    "  use_peft: true\n",
    "  lora:\n",
    "    r: 4\n",
    "    lora_alpha: 4\n",
    "    target_modules: [\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"]\n",
    "\n",
    "train:\n",
    "  optimizer:\n",
    "    learning_rate: 1e-5\n",
    "    adam_beta1: 0.9\n",
    "    adam_beta2: 0.999\n",
    "    adam_weight_decay: 1e-2\n",
    "    adam_epsilon: 1e-08\n",
    "  train_args:\n",
    "    resume_from_checkpoint:\n",
    "    per_device_train_batch_size: 4\n",
    "    gradient_accumulation_steps: 1\n",
    "    gradient_checkpointing: \n",
    "    num_train_epochs: 100\n",
    "    max_train_steps: \n",
    "    checkpointing_steps: \n",
    "    lr_scheduler_name: \"constant\"\n",
    "    max_grad_norm: 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-10T11:41:01.822Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile configs/generate_sd15_lora.yaml\n",
    "model:\n",
    "    model_name_or_path: \"sd-legacy/stable-diffusion-v1-5\"\n",
    "    lora_path: exps/ft_sd15_lora/results/lora_stable-diffusion-v1-5_rank4_s7300_r256_20250210-103454.safetensors\n",
    "    output_dir: \"./output_dir\"\n",
    "\n",
    "generate:\n",
    "  prompt: ['an icon of a phone', 'an icon of an laptop', 'an icon of a TV', 'an icon of a headphone', 'an icon of a earphone']\n",
    "  negative_prompt: \"low quality, blur, watermark, words, name\"\n",
    "  num_images_per_prompt: 4\n",
    "  generator_seed: 202502\n",
    "  width: 256\n",
    "  height: 256\n",
    "  guidance_scale: 8.5\n",
    "  scheduler: \"EulerDiscreteScheduler\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T07:25:50.236227Z",
     "iopub.status.busy": "2025-02-10T07:25:50.235988Z",
     "iopub.status.idle": "2025-02-10T07:25:50.255033Z",
     "shell.execute_reply": "2025-02-10T07:25:50.254242Z",
     "shell.execute_reply.started": "2025-02-10T07:25:50.236203Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "datasets==3.2.0\n",
    "accelerate==1.2.1\n",
    "peft==0.14.0\n",
    "trl==0.14.0\n",
    "bitsandbytes==0.45.1\n",
    "git+https://github.com/huggingface/transformers\n",
    "wandb==0.19.1\n",
    "omegaconf==2.3.0\n",
    "pyyaml==6.0.2\n",
    "hydra-core==1.3.2\n",
    "python-dotenv==1.0.1\n",
    "evaluate==0.4.3\n",
    "rouge_score==0.1.2\n",
    "py7zr==0.22.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!accelerate launch --num_processes=1 finetune.py --config_path configs/ft_sd15_lora.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-02-10T11:41:01.824Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!accelerate launch --num_processes=1 generate.py --config_path configs/generate_sd15_lora.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-10T10:47:46.101693Z",
     "iopub.status.busy": "2025-02-10T10:47:46.101387Z",
     "iopub.status.idle": "2025-02-10T10:47:46.220938Z",
     "shell.execute_reply": "2025-02-10T10:47:46.220174Z",
     "shell.execute_reply.started": "2025-02-10T10:47:46.101667Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lora_stable-diffusion-v1-5_rank4_s7300_r256_20250210-103454.safetensors\n"
     ]
    }
   ],
   "source": [
    "!ls /kaggle/working/exps/ft_sd15_lora/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6587644,
     "sourceId": 10639954,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
